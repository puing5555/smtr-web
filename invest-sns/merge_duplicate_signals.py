"""
ì½”ë¦°ì´ ì•„ë¹  ì‹œê·¸ë„ ì¤‘ë³µ í•©ì¹˜ê¸°
- 1ì˜ìƒ 1ì¢…ëª© 1ì‹œê·¸ë„ ì›ì¹™ ì ìš©
- ê°™ì€ video_id + ê°™ì€ assetì˜ ì‹œê·¸ë„ë“¤ì„ 1ê°œë¡œ í•©ì¹˜ê¸°
- ëŒ€í‘œ ì¸ìš©êµ¬, ìµœì¢… ë°©í–¥, ì¢…í•© ë§¥ë½ ìƒì„±
"""
import json
import os
import sys
import io
from collections import defaultdict
from datetime import datetime

# UTF-8 ì¶œë ¥ ì„¤ì •
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', line_buffering=True)
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', line_buffering=True)

def get_signal_priority(signal_type):
    """ì‹œê·¸ë„ íƒ€ì…ë³„ ìš°ì„ ìˆœìœ„ (ê°•í•œ ì‹ í˜¸ì¼ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)"""
    priority_map = {
        'STRONG_BUY': 8,
        'BUY': 7, 
        'POSITIVE': 6,
        'HOLD': 5,
        'NEUTRAL': 4,
        'CONCERN': 3,
        'SELL': 2,
        'STRONG_SELL': 1
    }
    return priority_map.get(signal_type, 0)

def get_confidence_score(confidence_str):
    """ì‹ ë¢°ë„ ë¬¸ìì—´ì„ ìˆ«ìë¡œ ë³€í™˜"""
    confidence_map = {
        'HIGH': 3,
        'MEDIUM': 2, 
        'LOW': 1
    }
    return confidence_map.get(confidence_str, 1)

def choose_dominant_signal(signals):
    """ì—¬ëŸ¬ ì‹œê·¸ë„ ì¤‘ ê°€ì¥ ì§€ë°°ì ì¸ ì‹œê·¸ë„ íƒ€ì… ê²°ì •"""
    if not signals:
        return 'NEUTRAL'
    
    # ê° ì‹œê·¸ë„ì˜ ê°€ì¤‘ì ìˆ˜ ê³„ì‚° (ìš°ì„ ìˆœìœ„ * ì‹ ë¢°ë„)
    signal_scores = defaultdict(float)
    
    for signal in signals:
        signal_type = signal.get('signal_type', 'NEUTRAL')
        confidence = signal.get('confidence', 'LOW')
        
        priority_score = get_signal_priority(signal_type)
        confidence_score = get_confidence_score(confidence)
        
        # ê°€ì¤‘ì ìˆ˜ = ìš°ì„ ìˆœìœ„ * ì‹ ë¢°ë„ ê°€ì¤‘ì¹˜
        weighted_score = priority_score * confidence_score
        signal_scores[signal_type] += weighted_score
    
    # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì‹œê·¸ë„ íƒ€ì… ë°˜í™˜
    if signal_scores:
        dominant_signal = max(signal_scores.items(), key=lambda x: x[1])
        return dominant_signal[0]
    
    return 'NEUTRAL'

def choose_best_quote(signals):
    """ê°€ì¥ ëŒ€í‘œì ì¸ ì¸ìš©êµ¬ ì„ íƒ"""
    if not signals:
        return ""
    
    # ê¸¸ì´ê°€ ì ë‹¹í•˜ê³  êµ¬ì²´ì ì¸ ì¸ìš©êµ¬ ìš°ì„ 
    quotes_with_scores = []
    
    for signal in signals:
        content = signal.get('content', '').strip()
        if not content:
            continue
        
        # ì ìˆ˜ ê¸°ì¤€:
        # 1. ì ë‹¹í•œ ê¸¸ì´ (50-200ì)
        # 2. êµ¬ì²´ì  ì–¸ê¸‰ ("ì‚¬ë¼", "íŒ”ì•„ë¼", "ë§¤ìˆ˜", "ë§¤ë„" ë“±)
        # 3. ì‹œê·¸ë„ ê°•ë„
        
        score = 0
        length = len(content)
        
        # ê¸¸ì´ ì ìˆ˜
        if 50 <= length <= 200:
            score += 3
        elif 20 <= length <= 50:
            score += 2
        elif length > 200:
            score += 1
        
        # êµ¬ì²´ì  í‘œí˜„ ì ìˆ˜
        action_keywords = ['ì‚¬ë¼', 'íŒ”ì•„ë¼', 'ë§¤ìˆ˜', 'ë§¤ë„', 'ë‹´ì•„ë¼', 'ë“¤ê³ ê°€ë¼', 'ë¹¼ë¼', 'ì˜¬ì¸', 'ëª°ë¹µ']
        for keyword in action_keywords:
            if keyword in content:
                score += 2
                break
        
        # ì‹œê·¸ë„ ê°•ë„ ì ìˆ˜
        signal_type = signal.get('signal_type', 'NEUTRAL')
        score += get_signal_priority(signal_type) * 0.5
        
        quotes_with_scores.append((content, score))
    
    if quotes_with_scores:
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì¸ìš©êµ¬ ì„ íƒ
        best_quote = max(quotes_with_scores, key=lambda x: x[1])
        return best_quote[0]
    
    return signals[0].get('content', '')

def merge_contexts(signals):
    """ëª¨ë“  ë§¥ë½ ì •ë³´ë¥¼ ì¢…í•©"""
    contexts = []
    
    for signal in signals:
        context = signal.get('context', '').strip()
        if context and context not in contexts:
            contexts.append(context)
    
    return ' | '.join(contexts) if contexts else ""

def determine_final_confidence(signals, final_signal_type):
    """ìµœì¢… ì‹ ë¢°ë„ ê²°ì •"""
    if not signals:
        return 'LOW'
    
    # ìµœì¢… ì‹œê·¸ë„ê³¼ ì¼ì¹˜í•˜ëŠ” ì‹œê·¸ë„ë“¤ì˜ ì‹ ë¢°ë„ ê³ ë ¤
    matching_confidences = []
    
    for signal in signals:
        if signal.get('signal_type') == final_signal_type:
            confidence = signal.get('confidence', 'LOW')
            matching_confidences.append(get_confidence_score(confidence))
    
    if not matching_confidences:
        # ì¼ì¹˜í•˜ëŠ” ì‹œê·¸ë„ì´ ì—†ìœ¼ë©´ ëª¨ë“  ì‹œê·¸ë„ì˜ í‰ê·  ì‹ ë¢°ë„
        all_confidences = [get_confidence_score(s.get('confidence', 'LOW')) for s in signals]
        avg_confidence = sum(all_confidences) / len(all_confidences)
    else:
        # ì¼ì¹˜í•˜ëŠ” ì‹œê·¸ë„ì˜ í‰ê·  ì‹ ë¢°ë„
        avg_confidence = sum(matching_confidences) / len(matching_confidences)
    
    # ìˆ«ìë¥¼ ë‹¤ì‹œ ë¬¸ìì—´ë¡œ ë³€í™˜
    if avg_confidence >= 2.5:
        return 'HIGH'
    elif avg_confidence >= 1.5:
        return 'MEDIUM'
    else:
        return 'LOW'

def merge_duplicate_signals(signals):
    """ì¤‘ë³µ ì‹œê·¸ë„ í•©ì¹˜ê¸°"""
    print(f"ğŸ“Š ì›ë³¸ ì‹œê·¸ë„: {len(signals)}ê°œ")
    
    # video_id + assetë³„ë¡œ ê·¸ë£¹í™”
    grouped_signals = defaultdict(list)
    
    for i, signal in enumerate(signals):
        video_id = signal.get('video_id', '')
        asset = signal.get('asset', '').strip().lower()  # ëŒ€ì†Œë¬¸ì í†µì¼
        
        # ìì‚°ëª… ì •ê·œí™” (ìœ ì‚¬í•œ í‘œê¸° í†µí•©)
        asset_normalized = normalize_asset_name(asset)
        
        key = f"{video_id}||{asset_normalized}"
        signal['original_index'] = i  # ì›ë³¸ ì¸ë±ìŠ¤ ë³´ì¡´
        grouped_signals[key].append(signal)
    
    print(f"ğŸ”„ ê·¸ë£¹í™” ê²°ê³¼: {len(grouped_signals)}ê°œ ê·¸ë£¹")
    
    merged_signals = []
    merge_stats = {
        'total_groups': len(grouped_signals),
        'merged_groups': 0,
        'single_signal_groups': 0,
        'max_signals_in_group': 0,
        'total_original': len(signals),
        'total_merged': 0
    }
    
    for key, group_signals in grouped_signals.items():
        video_id, asset_normalized = key.split('||')
        
        if len(group_signals) == 1:
            # ë‹¨ì¼ ì‹œê·¸ë„ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
            merged_signal = group_signals[0].copy()
            merge_stats['single_signal_groups'] += 1
        else:
            # ë‹¤ì¤‘ ì‹œê·¸ë„ í•©ì¹˜ê¸°
            print(f"ğŸ”€ í•©ì¹˜ê¸°: {asset_normalized} in {video_id} - {len(group_signals)}ê°œ ì‹œê·¸ë„")
            merge_stats['merged_groups'] += 1
            merge_stats['max_signals_in_group'] = max(merge_stats['max_signals_in_group'], len(group_signals))
            
            # ê¸°ë³¸ ì •ë³´ëŠ” ì²« ë²ˆì§¸ ì‹œê·¸ë„ì—ì„œ
            base_signal = group_signals[0]
            
            # ìµœì¢… ì‹œê·¸ë„ íƒ€ì… ê²°ì •
            final_signal_type = choose_dominant_signal(group_signals)
            
            # ëŒ€í‘œ ì¸ìš©êµ¬ ì„ íƒ
            best_quote = choose_best_quote(group_signals)
            
            # ë§¥ë½ í•©ì¹˜ê¸°
            merged_context = merge_contexts(group_signals)
            
            # ìµœì¢… ì‹ ë¢°ë„ ê²°ì •
            final_confidence = determine_final_confidence(group_signals, final_signal_type)
            
            # ì›ë³¸ ì¸ë±ìŠ¤ë“¤ ê¸°ë¡
            original_indices = [s['original_index'] for s in group_signals]
            
            merged_signal = {
                'asset': base_signal.get('asset', ''),  # ì›ë³¸ í‘œê¸° ìœ ì§€
                'signal_type': final_signal_type,
                'content': best_quote,
                'confidence': final_confidence,
                'context': merged_context,
                'video_id': video_id,
                'title': base_signal.get('title', ''),
                'merged_from_indices': original_indices,
                'merged_from_count': len(group_signals),
                'original_signals': [
                    {
                        'signal_type': s.get('signal_type'),
                        'content': s.get('content', '')[:100] + '...' if len(s.get('content', '')) > 100 else s.get('content', ''),
                        'confidence': s.get('confidence')
                    } for s in group_signals
                ]
            }
        
        merged_signals.append(merged_signal)
    
    merge_stats['total_merged'] = len(merged_signals)
    
    print(f"âœ… í•©ì¹˜ê¸° ì™„ë£Œ:")
    print(f"   - ì›ë³¸: {merge_stats['total_original']}ê°œ")
    print(f"   - í•©ì¹œ ê²°ê³¼: {merge_stats['total_merged']}ê°œ") 
    print(f"   - ë‹¨ì¼ ê·¸ë£¹: {merge_stats['single_signal_groups']}ê°œ")
    print(f"   - í•©ì³ì§„ ê·¸ë£¹: {merge_stats['merged_groups']}ê°œ")
    print(f"   - ìµœëŒ€ í•©ì¹œ ê°œìˆ˜: {merge_stats['max_signals_in_group']}ê°œ")
    
    return merged_signals, merge_stats

def normalize_asset_name(asset_name):
    """ìì‚°ëª… ì •ê·œí™” (ìœ ì‚¬í•œ í‘œê¸° í†µí•©)"""
    asset_name = asset_name.lower().strip()
    
    # ê³µí†µ ì •ê·œí™” ê·œì¹™
    normalizations = {
        'ì´ë”ë¥¨': 'ì´ë”ë¦¬ì›€',
        'ethereum': 'ì´ë”ë¦¬ì›€',
        'bitcoin': 'ë¹„íŠ¸ì½”ì¸',
        'btc': 'ë¹„íŠ¸ì½”ì¸',
        'eth': 'ì´ë”ë¦¬ì›€',
        'xrp': 'xrp',
        'ripple': 'xrp',
        'ë¹„íŠ¸ë§ˆì¸': 'ë¹„íŠ¸ë§ˆì¸',
        'bitmine': 'ë¹„íŠ¸ë§ˆì¸',
        'bmnr': 'ë¹„íŠ¸ë§ˆì¸',
        'bmr': 'ë¹„íŠ¸ë§ˆì¸',
        'bm': 'ë¹„íŠ¸ë§ˆì¸',
        'ì¼„í†¤': 'ì¼„í†¤',
        'canton': 'ì¼„í†¤',
        'ccì½”ì¸': 'ccì½”ì¸',
        'cctoken': 'ccì½”ì¸'
    }
    
    for original, normalized in normalizations.items():
        if original in asset_name:
            return normalized
    
    return asset_name

def save_merged_signals(merged_signals, merge_stats):
    """í•©ì¹œ ì‹œê·¸ë„ ì €ì¥"""
    output_data = {
        'metadata': {
            'merge_timestamp': str(datetime.now()),
            'merge_rule': '1ì˜ìƒ 1ì¢…ëª© 1ì‹œê·¸ë„',
            'total_original': merge_stats['total_original'],
            'total_merged': merge_stats['total_merged'],
            'compression_ratio': f"{merge_stats['total_merged']}/{merge_stats['total_original']} ({(merge_stats['total_merged']/merge_stats['total_original']*100):.1f}%)"
        },
        'merge_stats': merge_stats,
        'signals': merged_signals
    }
    
    # í•©ì¹œ ì‹œê·¸ë„ ì €ì¥
    output_file = 'C:\\Users\\Mario\\work\\invest-sns\\smtr_data\\corinpapa1106\\_merged_signals.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ í•©ì¹œ ì‹œê·¸ë„ ì €ì¥: {output_file}")
    
    # ì‹œê·¸ë„ë§Œ ë”°ë¡œ ì €ì¥ (Claude ê²€ì¦ìš©)
    signal_only_file = 'C:\\Users\\Mario\\work\\invest-sns\\smtr_data\\corinpapa1106\\_merged_signals_only.json'  
    with open(signal_only_file, 'w', encoding='utf-8') as f:
        json.dump(merged_signals, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ ì‹œê·¸ë„ë§Œ ì €ì¥: {signal_only_file}")
    
    return output_file

def generate_merge_report(merged_signals, merge_stats):
    """í•©ì¹˜ê¸° ë³´ê³ ì„œ ìƒì„±"""
    
    # í•©ì³ì§„ ê·¸ë£¹ ë¶„ì„
    merged_groups = [s for s in merged_signals if s.get('merged_from_count', 1) > 1]
    
    report = f"""# ğŸ“Š ì½”ë¦°ì´ ì•„ë¹  ì‹œê·¸ë„ ì¤‘ë³µ í•©ì¹˜ê¸° ë³´ê³ ì„œ

## ğŸ¯ í•©ì¹˜ê¸° ê·œì¹™
- **1ì˜ìƒ 1ì¢…ëª© 1ì‹œê·¸ë„** ì›ì¹™ ì ìš©
- ê°™ì€ video_id + ê°™ì€ assetì˜ ì‹œê·¸ë„ë“¤ì„ 1ê°œë¡œ í†µí•©
- ê°€ì¥ ê°•í•œ ì‹œê·¸ë„ íƒ€ì… ìš°ì„  (STRONG_BUY > BUY > ... > STRONG_SELL)
- ëŒ€í‘œ ì¸ìš©êµ¬ ì„ íƒ (êµ¬ì²´ì ì´ê³  ëª…í™•í•œ í‘œí˜„ ìš°ì„ )
- ëª¨ë“  ë§¥ë½ ì •ë³´ ì¢…í•©

## ğŸ“ˆ í•©ì¹˜ê¸° ê²°ê³¼

### ì „ì²´ í†µê³„
- **ì›ë³¸ ì‹œê·¸ë„**: {merge_stats['total_original']}ê°œ
- **í•©ì¹œ ê²°ê³¼**: {merge_stats['total_merged']}ê°œ
- **ì••ì¶•ë¥ **: {(merge_stats['total_merged']/merge_stats['total_original']*100):.1f}%
- **ë‹¨ì¼ ì‹œê·¸ë„ ê·¸ë£¹**: {merge_stats['single_signal_groups']}ê°œ
- **í•©ì³ì§„ ê·¸ë£¹**: {merge_stats['merged_groups']}ê°œ

### ì£¼ìš” í†µí•© ì‚¬ë¡€ (ìƒìœ„ 10ê°œ)
"""
    
    # ê°€ì¥ ë§ì´ í•©ì³ì§„ ê·¸ë£¹ë“¤ í‘œì‹œ
    top_merged = sorted(merged_groups, key=lambda x: x.get('merged_from_count', 1), reverse=True)[:10]
    
    for i, signal in enumerate(top_merged, 1):
        asset = signal.get('asset', 'N/A')
        video_id = signal.get('video_id', 'N/A')
        count = signal.get('merged_from_count', 1)
        final_type = signal.get('signal_type', 'N/A')
        
        report += f"\n{i}. **{asset}** ({video_id}) - {count}ê°œ â†’ {final_type}\n"
        
        original_types = [s['signal_type'] for s in signal.get('original_signals', [])]
        if original_types:
            report += f"   ì›ë³¸: {' + '.join(original_types)}\n"
        
        content = signal.get('content', '')
        if content:
            report += f"   ëŒ€í‘œ ì¸ìš©: \"{content[:100]}{'...' if len(content) > 100 else ''}\"\n"

    # ì¢…ëª©ë³„ í†µê³„
    asset_counts = {}
    for signal in merged_signals:
        asset = signal.get('asset', 'UNKNOWN')
        asset_counts[asset] = asset_counts.get(asset, 0) + 1
    
    top_assets = sorted(asset_counts.items(), key=lambda x: x[1], reverse=True)[:10]
    
    report += f"""

### ì¢…ëª©ë³„ ì‹œê·¸ë„ ë¶„í¬ (ìƒìœ„ 10ê°œ)
"""
    
    for i, (asset, count) in enumerate(top_assets, 1):
        percentage = (count / len(merged_signals)) * 100
        report += f"{i}. **{asset}**: {count}ê°œ ({percentage:.1f}%)\n"

    # ì‹œê·¸ë„ íƒ€ì…ë³„ ë¶„í¬
    signal_type_counts = {}
    for signal in merged_signals:
        signal_type = signal.get('signal_type', 'UNKNOWN')
        signal_type_counts[signal_type] = signal_type_counts.get(signal_type, 0) + 1
    
    report += f"""

### ìµœì¢… ì‹œê·¸ë„ íƒ€ì… ë¶„í¬
"""
    
    for signal_type, count in sorted(signal_type_counts.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / len(merged_signals)) * 100
        report += f"- **{signal_type}**: {count}ê°œ ({percentage:.1f}%)\n"

    report += f"""

## ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„
1. í•©ì¹œ ì‹œê·¸ë„ {merge_stats['total_merged']}ê°œì— ëŒ€í•´ Claude ê²€ì¦ ì‹¤í–‰
2. íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ (1ì˜ìƒ 1ì¢…ëª©ë‹¹ 1ê°œë§Œ)
3. HTML ë¦¬ë·° í˜ì´ì§€ ì—…ë°ì´íŠ¸
4. ìµœì¢… ë³´ê³ ì„œ ìƒì„±

---
ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
    
    # ë³´ê³ ì„œ ì €ì¥
    report_file = "C:\\Users\\Mario\\work\\invest-sns\\MERGE_REPORT.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"ğŸ“„ í•©ì¹˜ê¸° ë³´ê³ ì„œ ìƒì„±: {report_file}")
    return report_file

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("ğŸ”€ ì½”ë¦°ì´ ì•„ë¹  ì‹œê·¸ë„ ì¤‘ë³µ í•©ì¹˜ê¸° ì‹œì‘")
    print("ğŸ“‹ ê·œì¹™: 1ì˜ìƒ 1ì¢…ëª© 1ì‹œê·¸ë„")
    
    # ì›ë³¸ ì‹œê·¸ë„ ë¡œë“œ
    signal_file = 'C:\\Users\\Mario\\work\\invest-sns\\smtr_data\\corinpapa1106\\_all_signals_194.json'
    
    if not os.path.exists(signal_file):
        print(f"âŒ ì›ë³¸ ì‹œê·¸ë„ íŒŒì¼ ì—†ìŒ: {signal_file}")
        return
    
    with open(signal_file, 'r', encoding='utf-8') as f:
        original_signals = json.load(f)
    
    # ì¤‘ë³µ í•©ì¹˜ê¸°
    merged_signals, merge_stats = merge_duplicate_signals(original_signals)
    
    # ê²°ê³¼ ì €ì¥
    output_file = save_merged_signals(merged_signals, merge_stats)
    
    # ë³´ê³ ì„œ ìƒì„±
    report_file = generate_merge_report(merged_signals, merge_stats)
    
    print(f"\nâœ… ì‹œê·¸ë„ í•©ì¹˜ê¸° ì™„ë£Œ!")
    print(f"ğŸ“Š ê²°ê³¼: {merge_stats['total_original']}ê°œ â†’ {merge_stats['total_merged']}ê°œ")
    print(f"ğŸ“ íŒŒì¼:")
    print(f"   - í•©ì¹œ ì‹œê·¸ë„: _merged_signals.json")
    print(f"   - ì‹œê·¸ë„ë§Œ: _merged_signals_only.json")
    print(f"   - ë³´ê³ ì„œ: MERGE_REPORT.md")

if __name__ == "__main__":
    main()