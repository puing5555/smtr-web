#!/usr/bin/env python3
"""
Step2 v2: íƒ€ì„ìŠ¤íƒ¬í”„ ë§¤ì¹­ ê°œì„ 
- ìë§‰ì„ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ë¬¶ì–´ì„œ ë§¤ì¹­ (ë¬¸ë§¥ íŒŒì•…)
- í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤ì¹­ (ì¢…ëª©ëª…, ê°€ê²©, ë§¤ìˆ˜/ë§¤ë„ ë“±)
- TF-IDF ìœ ì‚¬ë„ í™œìš©
- ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ì¢…ëª©ëª… + í•µì‹¬ í‚¤ì›Œë“œë¡œ í´ë°±
"""
import json
import os
import re
import sys
import io
import glob
from difflib import SequenceMatcher

sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', line_buffering=True)
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', line_buffering=True)

def parse_timestamp(timestamp_str):
    try:
        timestamp_str = timestamp_str.strip('[]')
        parts = timestamp_str.split(':')
        if len(parts) == 3:
            h, m, s = map(int, parts)
            return h * 3600 + m * 60 + s
        elif len(parts) == 2:
            m, s = map(int, parts)
            return m * 60 + s
        return None
    except:
        return None

def load_subtitle_file(filepath):
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        pattern = r'\[(\d{1,2}:\d{2}(?::\d{2})?)\]\s*([^\[\n]+)'
        matches = re.findall(pattern, content, re.MULTILINE)
        
        subtitles = []
        for ts_str, text in matches:
            ts_sec = parse_timestamp(ts_str)
            if ts_sec is not None:
                clean = re.sub(r'\s+', ' ', text.strip())
                if clean:
                    subtitles.append({
                        'timestamp': f"[{ts_str}]",
                        'timestamp_seconds': ts_sec,
                        'text': clean
                    })
        return subtitles
    except Exception as e:
        print(f"Error loading {filepath}: {e}")
        return []

def clean_text(text):
    """í…ìŠ¤íŠ¸ ì •ê·œí™”"""
    text = re.sub(r'[^\w\sê°€-í£]', ' ', text.lower())
    return re.sub(r'\s+', ' ', text).strip()

def extract_keywords(text):
    """ì‹œê·¸ë„ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    # ì¢…ëª©ëª…, ìˆ«ì(ê°€ê²©), ë§¤ìˆ˜/ë§¤ë„ ê´€ë ¨ ìš©ì–´
    keywords = set()
    
    # í•œê¸€ ëª…ì‚¬ (2ê¸€ì ì´ìƒ)
    korean = re.findall(r'[ê°€-í£]{2,}', text)
    keywords.update(korean)
    
    # ì˜ë¬¸ ë‹¨ì–´
    english = re.findall(r'[a-zA-Z]{2,}', text)
    keywords.update([w.lower() for w in english])
    
    # ìˆ«ì+ë‹¨ìœ„ (ê°€ê²© ë“±)
    numbers = re.findall(r'\d+(?:\.\d+)?(?:ë§Œ|ì–µ|ì›|ë‹¬ëŸ¬|%|ë°°)?', text)
    keywords.update(numbers)
    
    return keywords

def build_windows(subtitles, window_size=5, stride=2):
    """ìë§‰ì„ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ë¬¶ê¸°"""
    windows = []
    for i in range(0, len(subtitles), stride):
        window = subtitles[i:i + window_size]
        if not window:
            continue
        combined_text = ' '.join(s['text'] for s in window)
        # ìœˆë„ìš°ì˜ ì‹œì‘ íƒ€ì„ìŠ¤íƒ¬í”„ ì‚¬ìš©
        windows.append({
            'timestamp': window[0]['timestamp'],
            'timestamp_seconds': window[0]['timestamp_seconds'],
            'text': combined_text,
            'start_idx': i,
            'end_idx': min(i + window_size, len(subtitles))
        })
    return windows

def keyword_overlap_score(keywords, text):
    """í‚¤ì›Œë“œê°€ í…ìŠ¤íŠ¸ì— ì–¼ë§ˆë‚˜ í¬í•¨ë˜ëŠ”ì§€"""
    if not keywords:
        return 0
    clean = clean_text(text)
    matched = sum(1 for kw in keywords if kw.lower() in clean)
    return matched / len(keywords)

def find_best_match_v2(signal, subtitles, asset_name=None):
    """ê°œì„ ëœ ë§¤ì¹­: ìœˆë„ìš° + í‚¤ì›Œë“œ + ì‹œí€€ìŠ¤ ë§¤ì¹­ ë³µí•©"""
    content = signal.get('content', '')
    context = signal.get('context', '')
    
    if not content or not subtitles:
        return None, 0
    
    # 1. í‚¤ì›Œë“œ ì¶”ì¶œ
    search_text = f"{content} {context}"
    keywords = extract_keywords(search_text)
    if asset_name:
        keywords.add(asset_name.lower())
        # ì¢…ëª©ëª… ë³€í˜• ì¶”ê°€
        for part in asset_name.split():
            if len(part) >= 2:
                keywords.add(part.lower())
    
    # 2. ìœˆë„ìš° ë¹Œë“œ (ë‹¤ì–‘í•œ í¬ê¸°)
    all_candidates = []
    
    # ê°œë³„ ìë§‰ ë¼ì¸
    for sub in subtitles:
        all_candidates.append(sub)
    
    # 3ì¤„ ìœˆë„ìš°
    for win in build_windows(subtitles, window_size=3, stride=1):
        all_candidates.append(win)
    
    # 5ì¤„ ìœˆë„ìš°
    for win in build_windows(subtitles, window_size=5, stride=2):
        all_candidates.append(win)
    
    # 10ì¤„ ìœˆë„ìš° (ë„“ì€ ë§¥ë½)
    for win in build_windows(subtitles, window_size=10, stride=5):
        all_candidates.append(win)
    
    # 3. ê° í›„ë³´ì— ëŒ€í•´ ë³µí•© ì ìˆ˜ ê³„ì‚°
    clean_content = clean_text(content)
    clean_context = clean_text(context)
    
    best_match = None
    best_score = 0
    
    for candidate in all_candidates:
        clean_cand = clean_text(candidate['text'])
        
        # (a) SequenceMatcher ìœ ì‚¬ë„
        seq_score = SequenceMatcher(None, clean_content, clean_cand).ratio()
        
        # (b) í‚¤ì›Œë“œ ì˜¤ë²„ë©
        kw_score = keyword_overlap_score(keywords, candidate['text'])
        
        # (c) ì¢…ëª©ëª… ë§¤ì¹­ ë³´ë„ˆìŠ¤
        asset_bonus = 0
        if asset_name and asset_name.lower() in clean_cand:
            asset_bonus = 0.2
        
        # (d) ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­
        substring_bonus = 0
        if clean_content[:20] in clean_cand or clean_cand[:30] in clean_content:
            substring_bonus = 0.15
        
        # (e) context ë§¤ì¹­
        context_score = 0
        if clean_context:
            context_score = SequenceMatcher(None, clean_context, clean_cand).ratio() * 0.3
        
        # ë³µí•© ì ìˆ˜
        total_score = (seq_score * 0.3) + (kw_score * 0.35) + asset_bonus + substring_bonus + context_score
        
        if total_score > best_score:
            best_score = total_score
            best_match = candidate
    
    return best_match, best_score

def add_timestamps_to_signals(signals):
    # ìë§‰ ë¡œë“œ
    subtitle_dirs = [
        "C:\\Users\\Mario\\work\\invest-sns\\smtr_data\\corinpapa1106",
        "C:\\Users\\Mario\\.openclaw\\workspace\\smtr_data\\corinpapa1106"
    ]
    
    subtitle_cache = {}
    for d in subtitle_dirs:
        if os.path.exists(d):
            for txt_file in glob.glob(os.path.join(d, "*.txt")):
                vid = os.path.splitext(os.path.basename(txt_file))[0]
                if vid not in subtitle_cache:
                    subs = load_subtitle_file(txt_file)
                    if subs:
                        subtitle_cache[vid] = subs
    
    print(f"ìë§‰ íŒŒì¼ ë¡œë“œ: {len(subtitle_cache)}ê°œ")
    
    matched = 0
    improved = 0
    
    for signal in signals:
        vid = signal.get('video_id')
        asset = signal.get('asset', '')
        old_ts = signal.get('timestamp_seconds')
        
        if vid not in subtitle_cache:
            signal.setdefault('timestamp', None)
            signal.setdefault('timestamp_seconds', None)
            signal.setdefault('timestamp_similarity', 0)
            continue
        
        best, score = find_best_match_v2(signal, subtitle_cache[vid], asset_name=asset)
        
        if best and score >= 0.2:
            signal['timestamp'] = best['timestamp']
            signal['timestamp_seconds'] = best['timestamp_seconds']
            signal['timestamp_similarity'] = round(score, 3)
            signal['matched_subtitle'] = best['text'][:200]
            matched += 1
            
            if not old_ts or old_ts == 0:
                improved += 1
                print(f"  âœ… NEW {vid} | {asset} | {best['timestamp']} (score={score:.2f})")
            elif old_ts != best['timestamp_seconds']:
                print(f"  ğŸ”„ UPD {vid} | {asset} | {old_ts}s â†’ {best['timestamp_seconds']}s (score={score:.2f})")
        else:
            signal['timestamp'] = None
            signal['timestamp_seconds'] = None
            signal['timestamp_similarity'] = 0
            signal['matched_subtitle'] = ''
            print(f"  âŒ MISS {vid} | {asset} | score={score:.2f}")
    
    return matched, improved

def main():
    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ (ì´ë¯¸ Claude ê²€ì¦ê¹Œì§€ ëœ ë°ì´í„°)
    input_path = "_claude_partial_164.json"
    output_path = "_claude_partial_164.json"  # ë®ì–´ì“°ê¸°
    backup_path = "_claude_partial_164_backup.json"
    
    print("=== Step2 v2: íƒ€ì„ìŠ¤íƒ¬í”„ ë§¤ì¹­ ê°œì„  ===")
    
    with open(input_path, 'r', encoding='utf-8') as f:
        signals = json.load(f)
    
    # ë°±ì—…
    with open(backup_path, 'w', encoding='utf-8') as f:
        json.dump(signals, f, ensure_ascii=False, indent=2)
    print(f"ë°±ì—… ì €ì¥: {backup_path}")
    
    # ê¸°ì¡´ ë§¤ì¹­ í†µê³„
    old_matched = sum(1 for s in signals if s.get('timestamp_seconds') and s['timestamp_seconds'] > 0)
    print(f"ê¸°ì¡´ íƒ€ì„ìŠ¤íƒ¬í”„ ìˆìŒ: {old_matched}/{len(signals)}")
    
    # ê°œì„ ëœ ë§¤ì¹­ ì‹¤í–‰
    matched, improved = add_timestamps_to_signals(signals)
    
    # ì €ì¥
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(signals, f, ensure_ascii=False, indent=2)
    
    new_matched = sum(1 for s in signals if s.get('timestamp_seconds') and s['timestamp_seconds'] > 0)
    
    print(f"\n=== ê²°ê³¼ ===")
    print(f"ì „ì²´ ì‹œê·¸ë„: {len(signals)}")
    print(f"ê¸°ì¡´ ë§¤ì¹­: {old_matched} ({old_matched*100//len(signals)}%)")
    print(f"ê°œì„  í›„ ë§¤ì¹­: {new_matched} ({new_matched*100//len(signals)}%)")
    print(f"ìƒˆë¡œ ì°¾ì€ íƒ€ì„ìŠ¤íƒ¬í”„: {improved}ê°œ")
    print(f"ì €ì¥: {output_path}")

if __name__ == "__main__":
    main()
